{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Principle: Trigonometry\n",
    "The idea here is simple, given (accurate, a big ask) metric depth values for every pixel's ray, allows us to calculate the length of any line segment in the 3D world which has its endpoints visible, using trigonometry. The depth values for the corresponding pixels give us the length of two sides of the triangle, and UniDepth's dense camera prediction directly gives us the angle between the two lines (without having to figure out the FOV).\n",
    "\n",
    "Armed with the length of two sides and the measure of their contained angle, I'm 99.999% sure we can compute the third side, although I've never been very good at trigonometry and keep forgetting the law of cosines.\n",
    "\n",
    "Unidepth makes this even easier for us, since it predicts rays completely using its pseudo-spherical output space, it can directly give us the world points corresponding to each pixel (calculated using its predicted camera parameters), which we can just calculate the euclidean distance between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import depth_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ASPDrive\\AcademicWork\\Sem7\\CV\\workspace\\project\\cv-project\\DepthPro\\src\\depth_pro\\depth_pro.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(config.checkpoint_uri, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "model, transform = depth_pro.create_model_and_transforms(device=dev)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I generate depth maps + camera preds for each guy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [09:16<00:00, 23.20s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "IMG_DIR = \"data\"\n",
    "OUT_DIR = \"depthpro_out\"\n",
    "\n",
    "depths = {}\n",
    "focals = {}\n",
    "\n",
    "for filename in tqdm([s for s in os.listdir(IMG_DIR) if s.endswith(\".jpg\")]):\n",
    "\tname = os.path.splitext(filename)[0]\n",
    "\t\n",
    "\timg, _, f_px = depth_pro.load_rgb(os.path.join(IMG_DIR, filename))\n",
    "\timg = transform(img).to(dev)\n",
    "\n",
    "\tpreds = model.infer(img)\n",
    "\n",
    "\tdepth = preds[\"depth\"].cpu().numpy()\n",
    "\t\n",
    "\tdepths[name] = depth\n",
    "\tfocals[name] = preds[\"focallength_px\"].cpu().item()\n",
    "\n",
    "\tplt.imsave(os.path.join(OUT_DIR, f\"{name}.png\"), depth, cmap=\"gray\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(f\"{OUT_DIR}/depths.npz\", **depths)\n",
    "np.savez(f\"{OUT_DIR}/focals.npz\", **focals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(2718.80249023)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focals = np.load(f\"{OUT_DIR}/focals.npz\")\n",
    "\n",
    "focals[\"kartripta1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, I want to talk about one neat benefit that Criminisi's method has over this one. Criminisi does not actually use any image data. It is a purely geometric derivation, and therefore, is not fazed by visual characteristics such as transparency, lighting conditions, etc etc. It is only concerned with projective invariants, and the only \"visual\" aspect of it is for identification of the keypoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, depth estimation, predictably, is very sensitive to these image characteristics, since it has nothing else to go off of. This results in outputs like these:\n",
    "\n",
    "<center>\n",
    "\t<img src = \"data/kartripta9.jpg\" style=\"width: 30%\">\n",
    "\t<img src = \"depthpro_out/kartripta9.png\" style=\"width: 30%\">\n",
    "</center>\n",
    "\n",
    "Clearly, it seems to register some of the glass wall as an actual solid wall, which means that this method won't work on this image. For Criminisi, however, this image is an ideal case, with the image plane at a high inclination angle to the world axis, we see it performing extremely well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-proj-dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
