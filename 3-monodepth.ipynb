{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Principle: Trigonometry\n",
    "The idea here is simple, given (accurate, a big ask) metric depth values for every pixel's ray, allows us to calculate the length of any line segment in the 3D world which has its endpoints visible, using trigonometry. The depth values for the corresponding pixels give us the length of two sides of the triangle, and UniDepth's dense camera prediction directly gives us the angle between the two lines (without having to figure out the FOV).\n",
    "\n",
    "Armed with the length of two sides and the measure of their contained angle, I'm 99.999% sure we can compute the third side, although I've never been very good at trigonometry and keep forgetting the law of cosines.\n",
    "\n",
    "Unidepth makes this even easier for us, since it predicts rays completely using its pseudo-spherical output space, it can directly give us the world points corresponding to each pixel (calculated using its predicted camera parameters), which we can just calculate the euclidean distance between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\cv-proj\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\miniconda3\\envs\\cv-proj\\lib\\site-packages\\xformers\\__init__.py\", line 55, in _is_triton_available\n",
      "    from xformers.triton.softmax import softmax as triton_softmax  # noqa\n",
      "  File \"c:\\ProgramData\\miniconda3\\envs\\cv-proj\\lib\\site-packages\\xformers\\triton\\softmax.py\", line 11, in <module>\n",
      "    import triton\n",
      "ModuleNotFoundError: No module named 'triton'\n",
      "Triton is not available, some optimizations will not be enabled.\n",
      "This is just a warning: triton is not available\n"
     ]
    }
   ],
   "source": [
    "from unidepth.models import UniDepthV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UniDepthV2.from_pretrained(\"lpiccinelli/unidepth-v2-vitl14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I generate depth maps + camera preds for each guy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]D:\\ASPDrive\\AcademicWork\\Sem7\\CV\\workspace\\project\\cv-project\\UniDepth\\unidepth\\models\\unidepthv2\\unidepthv2.py:47: UserWarning: Resolution level is not set. Using max resolution. You can tradeoff resolution for speed by setting a number in [0,10]. This can be achieved by setting model's `resolution_level` attribute.\n",
      "  warnings.warn(\n",
      "D:\\ASPDrive\\AcademicWork\\Sem7\\CV\\workspace\\project\\cv-project\\UniDepth\\unidepth\\layers\\attention.py:142: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  x = F.scaled_dot_product_attention(\n",
      "100%|██████████| 24/24 [00:50<00:00,  2.12s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "IMG_DIR = \"data\"\n",
    "OUT_DIR = \"unidepth_out\"\n",
    "\n",
    "depths = {}\n",
    "points = {}\n",
    "\n",
    "for filename in tqdm([s for s in os.listdir(IMG_DIR) if s.endswith(\".jpg\")]):\n",
    "\tname = os.path.splitext(filename)[0]\n",
    "\t\n",
    "\timg = np.array(Image.open(os.path.join(IMG_DIR, filename)))\n",
    "\timg_torch = torch.from_numpy(img).permute((2, 0, 1))\n",
    "\n",
    "\tpreds = model.infer(img_torch)\n",
    "\n",
    "\tdepth = np.fliplr(preds[\"depth\"][0].squeeze().cpu().numpy().transpose())\n",
    "\n",
    "\tdepths[name] = depth\n",
    "\tpoints[name] = preds[\"points\"].squeeze().cpu()\n",
    "\n",
    "\tplt.imsave(os.path.join(OUT_DIR, f\"{name}.png\"), depth, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(f\"{OUT_DIR}/depths.npz\", **depths)\n",
    "np.savez(f\"{OUT_DIR}/points.npz\", **points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3072, 4096)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = np.load(f\"{OUT_DIR}/points.npz\")\n",
    "\n",
    "points[\"kartripta1\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, I want to talk about one neat benefit that Criminisi's method has over this one. Criminisi does not actually use any image data. It is a purely geometric derivation, and therefore, is not fazed by visual characteristics such as transparency, lighting conditions, etc etc. It is only concerned with projective invariants, and the only \"visual\" aspect of it is for identification of the keypoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, depth estimation, predictably, is very sensitive to these image characteristics, since it has nothing else to go off of. This results in outputs like these:\n",
    "\n",
    "<center>\n",
    "\t<img src = \"data/kartripta9.jpg\" style=\"width: 30%\">\n",
    "\t<img src = \"unidepth_out/kartripta9.png\" style=\"width: 30%\">\n",
    "</center>\n",
    "\n",
    "Clearly, it seems to register some of the glass wall as an actual solid wall, which means that this method won't work on this image. For Criminisi, however, this image is an ideal case, with the image plane at a high inclination angle to the world axis, we see it performing extremely well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
